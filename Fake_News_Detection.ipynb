{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iqAi3VrKh9L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import *\n",
        "from wordcloud import STOPWORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"fake_or_real_news.csv\")\n",
        "data = data.drop('Unnamed: 0', axis=1)\n",
        "data = data.drop('title', axis=1)"
      ],
      "metadata": {
        "id": "rpGrGvrIvqUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_nums = []\n",
        "for i in data['label'].values:\n",
        "  if i == \"FAKE\":\n",
        "    label_nums.append(1)\n",
        "  else:\n",
        "    label_nums.append(2)\n",
        "\n",
        "data.insert(2, \"label num\", label_nums, True)"
      ],
      "metadata": {
        "id": "0DtDnc34LI3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "5vt1aN4dM5xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['text'].values\n",
        "y = data['label num'].values"
      ],
      "metadata": {
        "id": "2hjToq05M_Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=2)"
      ],
      "metadata": {
        "id": "KEZPT_ugL7nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "  stemmer = PorterStemmer()\n",
        "  preprocessed_data = []\n",
        "\n",
        "  for entry in data:\n",
        "    entry = word_tokenize(entry)\n",
        "\n",
        "    for word_index in range(len(entry)-1,-1,-1):\n",
        "      if entry[word_index].lower() in STOPWORDS:\n",
        "        entry.pop(word_index)\n",
        "      else:\n",
        "        entry[word_index] = stemmer.stem(entry[word_index])\n",
        "    \n",
        "    preprocessed_data.append(entry)\n",
        "\n",
        "  return preprocessed_data"
      ],
      "metadata": {
        "id": "IfJr4F5qP_mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = preprocess(X_train)\n",
        "train_labels = y_train"
      ],
      "metadata": {
        "id": "eAHbJKsiSTS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = preprocess(X_test)\n",
        "test_labels = y_test"
      ],
      "metadata": {
        "id": "HHUZP4MpbLOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = [\" \".join(t) for t in train_sentences]\n",
        "\n",
        "train_vectorizer = CountVectorizer()\n",
        "\n",
        "train_vectorizer.fit(train_sentences)\n",
        "train_vect = train_vectorizer.transform(train_sentences).toarray()\n",
        "  \n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(train_vect, train_labels)"
      ],
      "metadata": {
        "id": "gAHv1xJrdsUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sentences[0:5])\n",
        "print([\" \".join(t) for t in train_sentences[0:5]])\n",
        "print(train_labels[0:5])\n",
        "print([l for l in train_labels[0:5]])"
      ],
      "metadata": {
        "id": "U_Z-9zHHex-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\" \".join(t) for t in  test_sentences]\n",
        "  \n",
        "test_vect = train_vectorizer.transform(test_sentences).toarray()\n",
        "\n",
        "preds = model.predict(test_vect)\n",
        "  \n",
        "acc = metrics.accuracy_score(test_labels, preds)\n",
        "\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "BF2Kqi6vfMXc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}